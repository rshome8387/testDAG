from airflow import DAG
from airflow.providers.amazon.aws.operators.emr import EmrAddStepsOperator
from datetime import datetime,timedelta
import logging

#Default arguments for the DAG
default_args={
  'owner': 'airflow',
  'depends_on_past':False,
  'start_date': datetime(2024, 1, 10),
  'email_on_failure':False,
  'email_on_retry':False,
  'retries': 1,
  'retry_delay': timedelta(minutes=5),
}

# Creating a DAG for the Spark job on EMR
dag = DAG('spark_emr_job10-4', default_args=default_args,schedule_interval=timedelta(days=1))

#Step definition for the SPARK Job
spark_step = [
   {
     'Name': 'DataGenerator',
     'ActionOnFailure': 'CONTINUE',
     'HadoopJarStep': { 
     'Jar':'command-runner.jar',
     'Args': [
        'spark-submit',
        '--deploy-mode', 'cluster',
        '--class', 'com.quantexa.scriptrunner.QuantexaSparkScriptRunner',
         '--class', 'com.quantexa.scriptrunner.QuantexaSparkScriptRunner',
        '--conf', 'spark.sql.legacy.parquet.datetimeRebaseModeInRead=LEGACY',
        '--conf', 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY',
        '--conf', 'spark.shuffle.useOldFetchProtocol=true',
        '--conf', 'spark.driver.extraJavaOptions=-Dhdp.version=3.0.1.0-187 -Dscala.color',
        '--conf', 'spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35',
        '--conf', 'spark.yarn.maxAppAttempts=1',
        '--conf', 'spark.dynamicAllocation.enabled=false',
        '--conf', 'spark.sql.codegen.wholeStage=false',
        '--conf', 'spark.serializer=org.apache.spark.serializer.KryoSerializer',
        '--conf', 'spark.redaction.regex=(?i)secret|password|encrypt_key',
        '--conf', 'spark.yarn.appMasterEnv.HDFS_ROOT=s3://quantexa-emr-logs-bucket-575193579798/emr/scripts/quantexa/scratch/sri/ThroughAirflow',
        '--conf', 'spark.executorEnv.HDFS_ROOT=s3://quantexa-emr-logs-bucket-575193579798/emr/scripts/quantexa/scratch/sri/ThroughAirflow',
        '--conf', 'spark.yarn.appMasterEnv.BVD_HDFS_ROOT=s3://quantexa-emr-logs-bucket-575193579798/emr/scripts/quantexa/scratch/sri/ThroughAirflow',
        '--conf', 'spark.executorEnv.BVD_HDFS_ROOT=s3://quantexa-emr-logs-bucket-575193579798/emr/scripts/quantexa/scratch/sri/ThroughAirflow',
        '--conf', 'spark.yarn.appMasterEnv.ICIJ_HDFS_ROOT=s3://quantexa-emr-logs-bucket-575193579798/emr/scripts/quantexa/scratch/sri/ThroughAirflow',
        '--conf', 'spark.executorEnv.ICIJ_HDFS_ROOT=s3://quantexa-emr-logs-bucket-575193579798/emr/scripts/quantexa/scratch/sri/ThroughAirflow',
        '--jars', 's3://bucketname/emr/scripts/test-sparkscript/danske-markets-aml-data-generator-shadow-4.6.0-RC-2-projects.jar,s3://bucketname/emr/scripts/test-sparkscript/spark-avro_2.12-3.0.1.jar,s3://bucketname/emr/scripts/test-sparkscript/danske-markets-aml-data-generator-shadow-4.6.0-RC-2-dependency.jar','s3://bucketname/emr/scripts/test-sparkscript/danske-markets-aml-data-generator-shadow-4.6.0-RC-2-projects.jar',
        '-s', 'com.quantexa.danske.markets.aml.generators.AllDataGenerator',
        '-r', 'config',
        '-e', 'dev'
     ]
    }
  }
]
#Operator to add spark Step to an existing EMR Cluster
try:
    add_step= EmrAddStepsOperator(
    task_id='add_spark_step',
    job_flow_id='j-YK08BEURY4S2',
    #aws_conn_id='s3://quantexa-emr-logs-bucket-575193579798',
    steps=spark_step,
    dag=dag
)
except Exception as e:
    logging.error(f"Error in Adding EMR Steps:{e}")
    # Set the task in the DAG Add_step